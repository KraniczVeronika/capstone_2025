# -*- coding: utf-8 -*-
"""CapstoneProject_KraniczVeronika2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hdCzD6WFWK5mno8-FxIoyQGaOpJYsrZh

# Deep Learning Project

## Aim of this project

## The Dataset Features

## First steps - importing libraries and reading the file
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

# Load the data
df = pd.read_csv('/content/drive/My Drive/Multivariate_pollution.csv')

"""# Data Preparation and brief Exploration

##Get to know the dataset
"""

# First look at the head
display(df.head())

# Check info
print(df.info())

# Check desriptive statistics
print(df.describe())

# See columns
print(df.columns)

"""##Data cleaning"""

# Convert datetime
df['date'] = pd.to_datetime(df['date'])

# Verify the type with info
print(df.info())

# One-hot encoding for 'wnd_dir'
df = pd.get_dummies(df, columns=['wnd_dir'], drop_first=True)

# Display the first few rows to verify the changes
print(df.head())

# Check the shape
print(df.shape)

# Count the NaN values by column
print(df.isna().sum())

# Check duplicates
print(df.duplicated().any())
print(df.duplicated().sum())

"""## Mini Exploratory Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set style for the plots
sns.set(style="whitegrid")

# Scatterplot on Temperature vs Pollution
plt.figure(figsize=(8, 5))
sns.scatterplot(x=df['temp'], y=df['pollution'], hue=df['rain'], palette='coolwarm')
plt.title("Temperature vs Pollution Levels", fontsize=14)
plt.xlabel("Temperature")
plt.ylabel("Pollution")
plt.legend(title="Rain", loc="upper left")
plt.show()

# Monthly Average Pollution plot
df['month'] = df['date'].dt.month
monthly_avg_pollution = df.groupby('month')['pollution'].mean()

plt.figure(figsize=(10, 5))
monthly_avg_pollution.plot(kind='bar', color='green')
plt.title("Monthly Average Pollution Levels", fontsize=14)
plt.xlabel("Month")
plt.ylabel("Average Pollution (µg/m³)")
plt.xticks(range(12),
           ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
           rotation=45)
plt.show()

## Looking for outliers
df.plot.box(subplots=True, layout=(-1,4), figsize=(20,25));

# Time-series of Pollution plot
plt.figure(figsize=(15, 5))
plt.plot(df['date'], df['pollution'], color='blue', label='Pollution')
plt.title("Time Series of Pollution Levels", fontsize=14)
plt.xlabel("Date")
plt.ylabel("Pollution")
plt.legend()
plt.show()

# Distribution of Pollution Levels plot
plt.figure(figsize=(8, 5))
sns.histplot(df['pollution'], kde=True, bins=50, color='blue')
plt.title("Distribution of Pollution Levels", fontsize=14)
plt.xlabel("Pollution")
plt.ylabel("Frequency")
plt.show()

#Handling pollution and wnd_spd outliers
pollution_cap = df['pollution'].quantile(0.99)
wind_speed_cap = df['wnd_spd'].quantile(0.99)
df['pollution'] = df['pollution'].apply(lambda x: min(x, pollution_cap))
df['wnd_spd'] = df['wnd_spd'].apply(lambda x: min(x, wind_speed_cap))

# Filter the dataframe to include only numerical columns
numerical_df = df.select_dtypes(include=['float64', 'int64'])

# Correlation matrix and heatmap
plt.figure(figsize=(10, 6))

# Calculate correlation
correlation_matrix = numerical_df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap of Variables", fontsize=14)
plt.show()

"""# Creating train-test split"""

# Define the cutoff date for splitting the data
cutoff_date = '2014-01-01'

# Split the data into training and testing sets based on the date
train_data = df[df['date'] < cutoff_date]
test_data = df[df['date'] >= cutoff_date]

# Separate features and target variable
X_train = train_data.drop(columns=['pollution', 'date'])
y_train = train_data['pollution']

X_test = test_data.drop(columns=['pollution', 'date'])
y_test = test_data['pollution']

# Check the shapes of the resulting datasets
print(f"Training features shape: {X_train.shape}")
print(f"Test features shape: {X_test.shape}")
print(f"Training target shape: {y_train.shape}")
print(f"Test target shape: {y_test.shape}")

from sklearn.model_selection import train_test_split

# Use 20% of the training data for validation
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

"""## Normalizing with MinMaxScaler"""

# Normalize with MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Display
print("X_train_scaled range:", X_train_scaled.min(), "to", X_train_scaled.max())
print("X_val_scaled range:", X_val_scaled.min(), "to", X_val_scaled.max())
print("X_test_scaled range:", X_test_scaled.min(), "to", X_test_scaled.max())

"""#Deep Learning Model"""

# Create lagged features for 24-hour forecasting
lag_hours = 24
for lag in range(1, lag_hours + 1):
    df[f'pollution_lag_{lag}'] = df['pollution'].shift(lag)

# Drop rows with NaN values due to shifting
df_lagged = df.dropna()

# Define features and target variable
X = df_lagged.drop(columns=['pollution', 'date'])
y = df_lagged['pollution']

# Use the revised train-test split
train_data = df_lagged[df_lagged['date'] < '2014-01-01']
test_data = df_lagged[df_lagged['date'] >= '2014-01-01']

# Train-test split based on time (already described)
cutoff_date = '2014-01-01'
train_data = df[df['date'] < cutoff_date]
test_data = df[df['date'] >= cutoff_date]

X_train = train_data.drop(columns=['pollution', 'date'])
y_train = train_data['pollution']
X_test = test_data.drop(columns=['pollution', 'date'])
y_test = test_data['pollution']

# Check for NaN or infinite values
if X_train.isna().sum().sum() > 0 or np.isinf(X_train).sum().sum() > 0:
    print("NaN or infinite values exist in X_train. Please clean the data.")

import numpy as np

# Drop rows with NaN or infinite values
X_train = X_train.dropna()
y_train = y_train[X_train.index]
X_train = X_train[np.isfinite(X_train).all(axis=1)]
y_train = y_train[np.isfinite(X_train).all(axis=1)]

"""## Implement the Model"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization

# Build the Neural Network model
model = Sequential()
model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))
model.add(BatchNormalization())  # Added Batch Normalization to stabilize training
model.add(Dense(units=32, activation='relu'))
model.add(Dropout(0.2))  # Dropout to prevent overfitting
model.add(Dense(units=1))  # Output layer for regression

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import MinMaxScaler

# Drop 'date' and keep lagged features only
X = df.drop(columns=['pollution', 'date'])
y = df['pollution']

# Normalize the data
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the Neural Network model
model = Sequential()
model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(units=32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=1))  # Output layer for regression

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=5, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
loss = model.evaluate(X_test_scaled, y_test)
print(f"Test Loss (MSE): {loss}")

"""## Hyperparameter tuning"""

# Installing keras_tuner
!pip install keras-tuner --upgrade

from keras_tuner.tuners import RandomSearch

# Prepare data as before (using lagged features and scaled)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define a function to build the model
def build_model(hp):
    model = Sequential()
    # Tune the number of units in the first Dense layer
    hp_units = hp.Int('units', min_value=32, max_value=256, step=32)
    model.add(Dense(units=hp_units, activation='relu', input_dim=X_train.shape[1]))

    # Add another hidden layer with a tunable number of units
    hp_units_2 = hp.Int('units_2', min_value=32, max_value=128, step=32)
    model.add(Dense(units=hp_units_2, activation='relu'))

    # Tune dropout rate
    hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)
    model.add(Dropout(hp_dropout))

    # Output layer
    model.add(Dense(units=1))

    # Tune learning rate for the optimizer
    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  loss='mean_squared_error')
    return model

# Use RandomSearch for hyperparameter tuning
tuner = RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=5,
    executions_per_trial=1,
    directory='tuning_results',
    project_name='pollution_forecasting'
)

# Run the hyperparameter search
tuner.search(X_train_scaled, y_train, epochs=5, validation_split=0.2, batch_size=32)

# Get the optimal hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"""
The optimal number of units in the first layer is {best_hps.get('units')}.
The optimal number of units in the second layer is {best_hps.get('units_2')}.
The optimal dropout rate is {best_hps.get('dropout')}.
The optimal learning rate is {best_hps.get('learning_rate')}.
""")

# Build and train the model with the best hyperparameters
model = tuner.hypermodel.build(best_hps)
history = model.fit(X_train_scaled, y_train, epochs=5, validation_split=0.2, batch_size=32)

# Evaluate the model on the test set
loss = model.evaluate(X_test_scaled, y_test)
print(f"Test Loss (MSE): {loss}")

"""## Evaulation of my model"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer

# Define features and target variable using the same split as the deep learning model
X_train = train_data.drop(columns=['pollution', 'date'])
y_train = train_data['pollution']
X_test = test_data.drop(columns=['pollution', 'date'])
y_test = test_data['pollution']

# Impute NaN values using SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)

# Train a simple linear regression model as a baseline
baseline_model = LinearRegression()
baseline_model.fit(X_train, y_train)

# Predict on the test set
y_pred_baseline = baseline_model.predict(X_test)

# Calculate evaluation metrics for the baseline model
mse_baseline = mean_squared_error(y_test, y_pred_baseline)
mae_baseline = mean_absolute_error(y_test, y_pred_baseline)
r2_baseline = r2_score(y_test, y_pred_baseline)

# Print evaluation results
print(f"Baseline Model Test Loss (MSE): {mse_baseline}")
print(f"Baseline Model Test Loss (MAE): {mae_baseline}")
print(f"Baseline Model R² Score: {r2_baseline}")

"""### Evaluation metrics"""

# Deep Learning Model Evaluation Metrics

# Predict on the test set using the deep learning model
y_pred_dl = model.predict(X_test_scaled)

# Calculate evaluation metrics for the deep learning model
mse_dl = mean_squared_error(y_test, y_pred_dl)
mae_dl = mean_absolute_error(y_test, y_pred_dl)
r2_dl = r2_score(y_test, y_pred_dl)

# Print evaluation results for the deep learning model
print(f"Deep Learning Model Test Loss (MSE): {mse_dl}")
print(f"Deep Learning Model Test Loss (MAE): {mae_dl}")
print(f"Deep Learning Model R² Score: {r2_dl}")

"""#### Training and Validation Loss"""

import matplotlib.pyplot as plt

# Assuming 'history' is the history object returned by model.fit()
# Plot the training and validation loss over epochs
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.title('Training and Validation Loss over Epochs')
plt.legend()
plt.show()

"""##Saving the best model"""

from tensorflow.keras.callbacks import ModelCheckpoint

# Define a checkpoint callback to save the best performing model
checkpoint = ModelCheckpoint(
    filepath='best_performance_model.keras',  # Save the model in .keras format
    monitor='val_loss',  # Metric to monitor for improvements
    save_best_only=True,  # Save only the best model
    mode='min',
    verbose=1
)

# Train the model with the checkpoint callback
history = model.fit(
    X_train_scaled,
    y_train,
    epochs=5,
    batch_size=32,
    validation_split=0.2,
    callbacks=[checkpoint]
)